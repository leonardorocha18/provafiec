{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0decd0-b77d-41f7-acfd-b6b14c6f49cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.email import EmailOperator\n",
    "from airflow.providers.microsoft.mssql.hooks.mssql import MsSqlHook\n",
    "from airflow.utils.dates import days_ago\n",
    "import requests\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import pyodbc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import year, month, col, when\n",
    "from pathlib import Path\n",
    "from sqlalchemy import create_engine, types\n",
    "import tarfile\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "\n",
    "def extract_data():\n",
    "    # Configuração do WebDriver para rodar no Windows\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Executa sem abrir janela\n",
    "\n",
    "    # Instalação automática do ChromeDriver\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    # URL da página a ser acessada\n",
    "    url = \"https://web3.antaq.gov.br/ea/sense/download.html#pt\"\n",
    "    driver.get(url)\n",
    "\n",
    "    # Pasta onde os arquivos serão baixados\n",
    "    download_folder = \"dados_antaq\"\n",
    "\n",
    "    # Palavras-chave para filtrar os links de download\n",
    "    keywords = {\"Atracacao\", \"Carga\", \"CargaConteinerizada\"}\n",
    "    exclusion_keywords = {\"Regiao\"}\n",
    "\n",
    "    # Certifique-se de que a pasta de download existe\n",
    "    os.makedirs(download_folder, exist_ok=True)\n",
    "\n",
    "    def extract_file(file_path):\n",
    "        \"\"\"Descompacta arquivos .zip ou .tar.gz e exclui o arquivo compactado.\"\"\"\n",
    "        try:\n",
    "            if file_path.endswith(\".zip\"):\n",
    "                with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(download_folder)  # Extrai os arquivos na pasta de download\n",
    "            elif file_path.endswith(\".tar.gz\") or file_path.endswith(\".tar\"):\n",
    "                with tarfile.open(file_path, 'r:*') as tar_ref:\n",
    "                    tar_ref.extractall(download_folder)  # Extrai os arquivos na pasta de download\n",
    "            else:\n",
    "                print(f\"Formato não suportado: {file_path}\")\n",
    "                return\n",
    "\n",
    "            os.remove(file_path)  # Remove o arquivo compactado após extração\n",
    "            print(f\"Arquivo {file_path} descompactado e pasta zip removido com sucesso.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao descompactar {file_path}: {e}\")\n",
    "\n",
    "    def get_filtered_links(driver):\n",
    "        \"\"\"Obtém e filtra os links da página com base nas palavras-chave definidas.\"\"\"\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")  # Encontra todos os links da página\n",
    "        return [\n",
    "            link.get_attribute(\"href\") for link in links\n",
    "            if link.get_attribute(\"href\") and\n",
    "            any(keyword in link.get_attribute(\"href\") for keyword in keywords) and\n",
    "            not any(ex_keyword in link.get_attribute(\"href\") for ex_keyword in exclusion_keywords)\n",
    "        ]\n",
    "\n",
    "    def download_file(url):\n",
    "        \"\"\"Faz o download do arquivo a partir da URL informada.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, timeout=30)  # Faz a requisição do arquivo\n",
    "            response.raise_for_status()  # Verifica se houve erro na requisição\n",
    "\n",
    "            file_name = os.path.basename(url)  # Obtém o nome do arquivo a partir da URL\n",
    "            file_path = os.path.join(download_folder, file_name)  # Define o caminho completo do arquivo\n",
    "\n",
    "            with open(file_path, 'wb') as file:\n",
    "                file.write(response.content)  # Salva o conteúdo baixado no arquivo\n",
    "\n",
    "            print(f\"Arquivo {file_name} baixado com sucesso.\")\n",
    "            extract_file(file_path)  # Chama a função para descompactar o arquivo, se necessário\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Erro ao baixar {url}: {e}\")\n",
    "\n",
    "    def process_year(driver, year):\n",
    "        \"\"\"Coleta e baixa arquivos para um ano específico.\"\"\"\n",
    "        print(f\"Processando ano {year}...\")\n",
    "\n",
    "        try:\n",
    "            # Aguarda até que o elemento select (caixa de seleção de ano) esteja visível\n",
    "            select_box = WebDriverWait(driver, 10).until(\n",
    "                EC.visibility_of_element_located((By.TAG_NAME, \"select\"))\n",
    "            select_box.send_keys(str(year))  # Seleciona o ano desejado\n",
    "            time.sleep(6)  # Aguarda carregamento dos links após a seleção\n",
    "\n",
    "            links = get_filtered_links(driver)  # Obtém os links filtrados\n",
    "\n",
    "            if links:\n",
    "                print(f\"{len(links)} arquivos encontrados para {year}. Iniciando download...\")\n",
    "                for url in links:\n",
    "                    download_file(url)  # Baixa os arquivos encontrados\n",
    "            else:\n",
    "                print(f\"Nenhum link válido encontrado para {year}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar o ano {year}: {e}\")\n",
    "\n",
    "    # Loop para processar os anos de 2021 a 2023\n",
    "    for year in range(2021, 2024):\n",
    "        process_year(driver, year)\n",
    "\n",
    "\n",
    "def transform_data():\n",
    "    # Inicializar a sessão do Spark\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"ANTAQ ETL\") \\\n",
    "        .config(\"spark.jars\", \"/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/mssql-jdbc-12.2.0.jre8.jar\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Caminho para os arquivos\n",
    "    base_path = r\"C:\\Users\\leona\\dados_antaq\"\n",
    "\n",
    "    # Lista de anos para processar\n",
    "    anos = [2021, 2022, 2023]\n",
    "\n",
    "    # Lista para armazenar os DataFrames de atracação e tempos\n",
    "    atracacao_dfs = []\n",
    "    tempos_dfs = []\n",
    "\n",
    "    # Ler e processar os arquivos de atracação e tempos de atracação para cada ano\n",
    "    for ano in anos:\n",
    "        # Ler o arquivo de atracação com delimitador ';'\n",
    "        atracacao_path = os.path.join(base_path, f\"{ano}Atracacao.txt\")\n",
    "        atracacao_df = spark.read.csv(atracacao_path, header=True, inferSchema=True, sep=\";\")\n",
    "\n",
    "        # Adicionar coluna \"Ano\" e \"Mês\" em atracação a partir da coluna 'Data Atracação'\n",
    "        atracacao_df = atracacao_df.withColumn(\"Ano\", F.year(col(\"Data Atracação\"))) \\\n",
    "            .withColumn(\"Mês\", F.month(col(\"Data Atracação\")))\n",
    "        atracacao_dfs.append(atracacao_df)\n",
    "\n",
    "        # Ler o arquivo de tempos de atracação com delimitador ';'\n",
    "        tempos_path = os.path.join(base_path, f\"{ano}TemposAtracacao.txt\")\n",
    "        tempos_df = spark.read.csv(tempos_path, header=True, inferSchema=True, sep=\";\")\n",
    "\n",
    "        # Garantir que temos a coluna IDAtracacao para o join\n",
    "        tempos_dfs.append(tempos_df)\n",
    "\n",
    "    # Concatenar todos os DataFrames de atracação\n",
    "    atracacao_final_df = atracacao_dfs[0]\n",
    "    for df in atracacao_dfs[1:]:\n",
    "        atracacao_final_df = atracacao_final_df.union(df)\n",
    "\n",
    "    # Concatenar todos os DataFrames de tempos de atracação\n",
    "    tempos_final_df = tempos_dfs[0]\n",
    "    for df in tempos_dfs[1:]:\n",
    "        tempos_final_df = tempos_final_df.union(df)\n",
    "\n",
    "    # Realizar o join entre os DataFrames de atracação e tempos de atracação com base na coluna IDAtracacao\n",
    "    final_df = atracacao_final_df.join(tempos_final_df, \"IDAtracacao\", \"left\")\n",
    "\n",
    "    # Selecionar as colunas desejadas para a tabela final\n",
    "    atracacao_final_df = final_df.select(\n",
    "        \"IDAtracacao\",\n",
    "        \"CDTUP\",\n",
    "        \"IDBerco\",\n",
    "        \"Berço\",\n",
    "        \"Porto Atracação\",\n",
    "        \"Apelido Instalação Portuária\",\n",
    "        \"Complexo Portuário\",\n",
    "        \"Tipo da Autoridade Portuária\",\n",
    "        F.to_timestamp(col(\"Data Atracação\"), \"dd/MM/yyyy HH:mm:ss\").alias(\"Data Atracação\"),\n",
    "        F.to_timestamp(col(\"Data Chegada\"), \"dd/MM/yyyy HH:mm:ss\").alias(\"Data Chegada\"),\n",
    "        F.to_timestamp(col(\"Data Desatracação\"), \"dd/MM/yyyy HH:mm:ss\").alias(\"Data Desatracação\"),\n",
    "        F.to_timestamp(col(\"Data Início Operação\"), \"dd/MM/yyyy HH:mm:ss\").alias(\"Data Início Operação\"),\n",
    "        F.to_timestamp(col(\"Data Término Operação\"), \"dd/MM/yyyy HH:mm:ss\").alias(\"Data Término Operação\"),\n",
    "        F.year(F.to_timestamp(col(\"Data Início Operação\"), \"dd/MM/yyyy HH:mm:ss\")).alias(\"Ano da data de início da operação\"),\n",
    "        F.lpad(F.month(F.to_timestamp(col(\"Data Início Operação\"), \"dd/MM/yyyy HH:mm:ss\")).cast(\"string\"), 2, \"0\").alias(\"Mês da data de início da operação\"),\n",
    "        \"Tipo de Operação\",\n",
    "        \"Tipo de Navegação da Atracação\",\n",
    "        \"Nacionalidade do Armador\",\n",
    "        \"FlagMCOperacaoAtracacao\",\n",
    "        \"Terminal\",\n",
    "        \"Município\",\n",
    "        \"UF\",\n",
    "        \"SGUF\",\n",
    "        \"Região Geográfica\",\n",
    "        \"Nº da Capitania\",\n",
    "        \"Nº do IMO\",\n",
    "        \"TEsperaAtracacao\",\n",
    "        \"TesperaInicioOp\",\n",
    "        \"TOperacao\",\n",
    "        \"TEsperaDesatracacao\",\n",
    "        \"TAtracado\",\n",
    "        \"TEstadia\"\n",
    "    )\n",
    "\n",
    "    # Caminho para os arquivos\n",
    "    base_path = r\"C:\\Users\\leona\\dados_antaq\"\n",
    "\n",
    "    # Listar os anos a serem processados\n",
    "    anos = [2021, 2022, 2023]\n",
    "\n",
    "    # Lista para armazenar os DataFrames de carga e outras tabelas\n",
    "    carga_dfs = []\n",
    "    atracacao_dfs = []\n",
    "    carga_conteinerizada_dfs = []\n",
    "\n",
    "    # Ler e processar os arquivos de carga, atracação, carga conteinerizada\n",
    "    for ano in anos:\n",
    "        # Ler o arquivo de carga\n",
    "        carga_path = os.path.join(base_path, f\"{ano}Carga.txt\")\n",
    "        carga_df = spark.read.csv(carga_path, header=True, inferSchema=True, sep=\";\")\n",
    "\n",
    "        # Verificar se a carga é conteinerizada\n",
    "        carga_df = carga_df.withColumn(\"Carga_Conteinerizada\",\n",
    "                                       when(col(\"Carga Geral Acondicionamento\") == \"Conteinerizada\", True).otherwise(False))\n",
    "\n",
    "        # Carregar a tabela CargaConteinerizada\n",
    "        carga_conteinerizada_path = os.path.join(base_path, f\"{ano}Carga_Conteinerizada.txt\")\n",
    "        carga_conteinerizada_df = spark.read.csv(carga_conteinerizada_path, header=True, inferSchema=True, sep=\";\")\n",
    "\n",
    "        # Unir a tabela Carga com a CargaConteinerizada (quando for conteinerizada)\n",
    "        carga_df = carga_df.join(carga_conteinerizada_df, carga_df[\"IDCarga\"] == carga_conteinerizada_df[\"IDCarga\"], \"left\") \\\n",
    "            .select(carga_df[\"*\"], carga_conteinerizada_df[\"CDMercadoriaConteinerizada\"], carga_conteinerizada_df[\"VLPesoCargaConteinerizada\"])\n",
    "\n",
    "        # Para carga conteinerizada, o Peso líquido será o Peso sem contêiner\n",
    "        carga_df = carga_df.withColumn(\"Peso_liquido\",\n",
    "                                       when(col(\"Carga_Conteinerizada\") == True, col(\"VLPesoCargaConteinerizada\"))\n",
    "                                       .otherwise(col(\"VLPesoCargaBruta\")))\n",
    "\n",
    "        # Alterar a coluna \"CDMercadoria\" para considerar os códigos das mercadorias da CargaConteinerizada\n",
    "        carga_df = carga_df.withColumn(\"CDMercadoria\",\n",
    "                                       when(col(\"Carga_Conteinerizada\") == True, col(\"CDMercadoriaConteinerizada\"))\n",
    "                                       .otherwise(col(\"CDMercadoria\")))\n",
    "\n",
    "        # Remover a coluna \"CDMercadoriaConteinerizada\", pois queremos apenas \"CDMercadoria\"\n",
    "        carga_df = carga_df.drop(\"CDMercadoriaConteinerizada\")\n",
    "\n",
    "        # Adicionar a tabela de carga processada à lista\n",
    "        carga_dfs.append(carga_df)\n",
    "\n",
    "    # Concatenar todos os DataFrames de carga\n",
    "    carga_final_df = carga_dfs[0]\n",
    "    for df in carga_dfs[1:]:\n",
    "        carga_final_df = carga_final_df.union(df)\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    # Criar uma sessão do Spark\n",
    "    spark = SparkSession.builder.appName(\"Download Parquet\").getOrCreate()\n",
    "\n",
    "    # Salvar atracacao_fato como Parquet\n",
    "    atracacao_final_df.write.mode(\"overwrite\").parquet(\"atracacao_fato.parquet\")\n",
    "\n",
    "    # Salvar carga_fato como Parquet\n",
    "    carga_final_df.write.mode(\"overwrite\").parquet(\"carga_fato.parquet\")\n",
    "\n",
    "    # Remover arquivos .crc antes de compactar\n",
    "    for crc_file in Path(\"atracacao_fato.parquet\").rglob(\"*.crc\"):\n",
    "        crc_file.unlink()\n",
    "    for crc_file in Path(\"carga_fato.parquet\").rglob(\"*.crc\"):\n",
    "        crc_file.unlink()\n",
    "\n",
    "    # Compactar apenas os arquivos parquet\n",
    "    shutil.make_archive(\"atracacao_fato\", 'zip', \"atracacao_fato.parquet\")\n",
    "    shutil.make_archive(\"carga_fato\", 'zip', \"carga_fato.parquet\")\n",
    "\n",
    "    # Caminhos dos arquivos ZIP\n",
    "    caminho_carga = r\"C:\\Users\\leona\\carga_fato.zip\"\n",
    "    caminho_atracacao = r\"C:\\Users\\leona\\atracacao_fato.zip\"\n",
    "\n",
    "    # Caminhos de destino para descompactação\n",
    "    caminho_destino_carga = r\"C:\\Users\\leona\\carga\"\n",
    "    caminho_destino_atracacao = r\"C:\\Users\\leona\\atracacao\"\n",
    "\n",
    "    # Verifica se os diretórios de destino existem; se não, cria os diretórios\n",
    "    if not os.path.exists(caminho_destino_carga):\n",
    "        os.makedirs(caminho_destino_carga)\n",
    "\n",
    "    if not os.path.exists(caminho_destino_atracacao):\n",
    "        os.makedirs(caminho_destino_atracacao)\n",
    "\n",
    "    # Função para descompactar e carregar os dados Parquet\n",
    "    def descompactar_e_carregar_parquet(caminho_zip, caminho_destino, nome_arquivo):\n",
    "        with zipfile.ZipFile(caminho_zip, 'r') as zip_ref:\n",
    "            # Extrai todos os arquivos para o diretório de destino\n",
    "            zip_ref.extractall(caminho_destino)\n",
    "            print(f\"{nome_arquivo} extraído com sucesso para: {caminho_destino}\")\n",
    "\n",
    "            # Encontra o arquivo Parquet dentro do diretório extraído\n",
    "            arquivos = zip_ref.namelist()\n",
    "            arquivo_parquet = [f for f in arquivos if f.endswith('.parquet')][0]  # Pega o primeiro arquivo Parquet\n",
    "            caminho_parquet = os.path.join(caminho_destino, arquivo_parquet)\n",
    "\n",
    "            # Carrega o arquivo Parquet usando pandas\n",
    "            df = pd.read_parquet(caminho_parquet)\n",
    "\n",
    "            return df\n",
    "\n",
    "    # Descompacta e carrega os dados\n",
    "    df_carga = descompactar_e_carregar_parquet(caminho_carga, caminho_destino_carga, \"carga_fato.zip\")\n",
    "    df_atracacao = descompactar_e_carregar_parquet(caminho_atracacao, caminho_destino_atracacao, \"atracacao_fato.zip\")\n",
    "\n",
    "\n",
    "def load_table_a():\n",
    "    # Caminho da pasta onde estão os arquivos\n",
    "    pasta_atracacao = r\"C:\\Users\\leona\\atracacao\"\n",
    "\n",
    "    # Encontra o arquivo Parquet na pasta\n",
    "    arquivos = os.listdir(pasta_atracacao)\n",
    "    arquivo_parquet = [f for f in arquivos if f.endswith('.parquet')][0]  # Pega o primeiro arquivo Parquet\n",
    "    caminho_parquet_atracacao = os.path.join(pasta_atracacao, arquivo_parquet)\n",
    "\n",
    "    # Configurações de conexão\n",
    "    server = 'localhost'  # Nome do servidor SQL Server\n",
    "    database = 'master'  # Nome do banco de dados\n",
    "    username = 'leo'  # Nome de usuário\n",
    "    password = '12345'  # Senha\n",
    "\n",
    "    # String de conexão\n",
    "connection_string = f\"mssql+pyodbc://{username}:{password}@{server}/{database}?driver=ODBC+Driver+17+for+SQL+Server\"\n",
    "\n",
    "# Cria a engine de conexão com o SQL Server\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Lê o arquivo Parquet\n",
    "df_atracacao = pd.read_parquet(caminho_parquet_atracacao)\n",
    "\n",
    "# Função para limpar valores discrepantes\n",
    "def limpar_valores_discrepantes(df):\n",
    "    # Substitui \"Valor Discrepante\" por NaN\n",
    "    df.replace(\"Valor Discrepante\", np.nan, inplace=True)\n",
    "    \n",
    "    # Converte colunas numéricas para o tipo correto\n",
    "    colunas_numericas = [\n",
    "        'TEsperaAtracacao', 'TesperaInicioOp', 'TOperacao', \n",
    "        'TEsperaDesatracacao', 'TAtracado', 'TEstadia'\n",
    "    ]\n",
    "    for coluna in colunas_numericas:\n",
    "        df[coluna] = pd.to_numeric(df[coluna], errors='coerce')  # Converte para numérico, inválidos viram NaN\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Limpa os dados\n",
    "df_atracacao = limpar_valores_discrepantes(df_atracacao)\n",
    "\n",
    "# Define o mapeamento de tipos de colunas para o SQL Server\n",
    "dtype_mapping = {\n",
    "    'IDAtracacao': types.INTEGER(),\n",
    "    'CDTUP': types.VARCHAR(length=50),\n",
    "    'IDBerco': types.VARCHAR(length=50),\n",
    "    'Berco': types.VARCHAR(length=100),\n",
    "    'PortoAtracacao': types.VARCHAR(length=100),\n",
    "    'ApelidoInstalacaoPortuaria': types.VARCHAR(length=100),\n",
    "    'ComplexoPortuario': types.VARCHAR(length=100),\n",
    "    'TipoAutoridadePortuaria': types.VARCHAR(length=100),\n",
    "    'DataAtracacao': types.DATETIME(),\n",
    "    'DataChegada': types.DATETIME(),\n",
    "    'DataDesatracacao': types.DATETIME(),\n",
    "    'DataInicioOperacao': types.DATETIME(),\n",
    "    'DataTerminoOperacao': types.DATETIME(),\n",
    "    'AnoDataInicioOperacao': types.INTEGER(),\n",
    "    'MesDataInicioOperacao': types.VARCHAR(length=50),\n",
    "    'TipoOperacao': types.VARCHAR(length=100),\n",
    "    'TipoNavegacaoAtracacao': types.VARCHAR(length=100),\n",
    "    'NacionalidadeArmador': types.INTEGER(),\n",
    "    'FlagMCOperacaoAtracacao': types.INTEGER(),\n",
    "    'Terminal': types.VARCHAR(length=100),\n",
    "    'Municipio': types.VARCHAR(length=100),\n",
    "    'UF': types.VARCHAR(length=50),\n",
    "    'SGUF': types.VARCHAR(length=50),\n",
    "    'RegiaoGeografica': types.VARCHAR(length=100),\n",
    "    'NumeroCapitania': types.VARCHAR(length=50),\n",
    "    'NumeroIMO': types.INTEGER(),\n",
    "    'TEsperaAtracacao': types.FLOAT(),\n",
    "    'TesperaInicioOp': types.FLOAT(),\n",
    "    'TOperacao': types.FLOAT(),\n",
    "    'TEsperaDesatracacao': types.FLOAT(),\n",
    "    'TAtracado': types.FLOAT(),\n",
    "    'TEstadia': types.FLOAT()\n",
    "}\n",
    "\n",
    "# Nome da tabela no SQL Server\n",
    "table_name = 'atracacao_fato'\n",
    "\n",
    "# Cria a tabela no SQL Server (se não existir) e insere os dados\n",
    "try:\n",
    "    # Conecta ao banco de dados\n",
    "    with engine.connect() as connection:\n",
    "        # Cria a tabela\n",
    "        df_atracacao.head(0).to_sql(\n",
    "            name=table_name,\n",
    "            con=connection,\n",
    "            if_exists='replace',\n",
    "            index=False,\n",
    "            dtype=dtype_mapping\n",
    "        )\n",
    "        print(f\"Tabela '{table_name}' criada com sucesso.\")\n",
    "\n",
    "        # Insere os dados na tabela\n",
    "        df_atracacao.to_sql(\n",
    "            name=table_name,\n",
    "            con=connection,\n",
    "            if_exists='append',\n",
    "            index=False\n",
    "        )\n",
    "        print(f\"Dados inseridos na tabela '{table_name}' com sucesso.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao inserir dados no SQL Server: {e}\")\n",
    "def load_table_c():\n",
    "    # Caminho da pasta onde estão os arquivos\n",
    "    pasta_carga = r\"C:\\Users\\leona\\carga\"\n",
    "\n",
    "    # Encontra o arquivo Parquet na pasta\n",
    "    arquivos = os.listdir(pasta_carga)\n",
    "    arquivo_parquet = [f for f in arquivos if f.endswith('.parquet')][0]  # Pega o primeiro arquivo Parquet\n",
    "    caminho_parquet_carga = os.path.join(pasta_carga, arquivo_parquet)\n",
    "\n",
    "    # Configurações de conexão\n",
    "    server = 'localhost'  # Nome do servidor SQL Server\n",
    "    database = 'master'   # Nome do banco de dados\n",
    "    username = 'leo'      # Nome de usuário\n",
    "    password = '12345'    # Senha\n",
    "\n",
    "    # String de conexão\n",
    "    connection_string = f\"mssql+pyodbc://{username}:{password}@{server}/{database}?driver=ODBC+Driver+17+for+SQL+Server\"\n",
    "\n",
    "    # Cria a engine de conexão com o SQL Server\n",
    "    engine = create_engine(connection_string)\n",
    "\n",
    "    # Lê o arquivo Parquet\n",
    "    df_carga = pd.read_parquet(caminho_parquet_carga)\n",
    "\n",
    "    # Função para limpar e converter os dados\n",
    "    def limpar_e_converter_dados(df):\n",
    "        # Substitui \"Valor Discrepante\" por NaN\n",
    "        df.replace(\"Valor Discrepante\", np.nan, inplace=True)\n",
    "        \n",
    "        # Converte colunas numéricas (que estão como string) para o tipo correto\n",
    "        colunas_numericas = ['TEU', 'VLPesoCargaBruta', 'VLPesoCargaConteinerizada', 'Peso_liquido']\n",
    "        for coluna in colunas_numericas:\n",
    "            df[coluna] = pd.to_numeric(df[coluna], errors='coerce')  # Converte para numérico, inválidos viram NaN\n",
    "        \n",
    "        # Converte colunas de flags para inteiros (se forem strings)\n",
    "        colunas_flags = ['FlagAutorizacao', 'FlagConteinerTamanho']\n",
    "        for coluna in colunas_flags:\n",
    "            if df[coluna].dtype == 'object':  # Verifica se a coluna é do tipo string\n",
    "                df[coluna] = pd.to_numeric(df[coluna], errors='coerce').fillna(0).astype(int)\n",
    "        \n",
    "        # Converte coluna booleana\n",
    "        if 'Carga_Conteinerizada' in df.columns:\n",
    "            df['Carga_Conteinerizada'] = df['Carga_Conteinerizada'].astype(bool)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    # Limpa e converte os dados\n",
    "    df_carga = limpar_e_converter_dados(df_carga)\n",
    "\n",
    "    # Define o mapeamento de tipos de colunas para o SQL Server\n",
    "    dtype_mapping = {\n",
    "        'IDCarga': types.INTEGER(),\n",
    "        'IDAtracacao': types.INTEGER(),\n",
    "        'Origem': types.VARCHAR(length=100),\n",
    "        'Destino': types.VARCHAR(length=100),\n",
    "        'CDMercadoria': types.VARCHAR(length=50),\n",
    "        'Tipo Operação da Carga': types.VARCHAR(length=100),\n",
    "        'Carga Geral Acondicionamento': types.VARCHAR(length=100),\n",
    "        'ConteinerEstado': types.VARCHAR(length=50),\n",
    "        'Tipo Navegação': types.VARCHAR(length=100),\n",
    "        'FlagAutorizacao': types.INTEGER(),\n",
    "        'FlagCabotagem': types.INTEGER(),\n",
    "        'FlagCabotagemMovimentacao': types.INTEGER(),\n",
    "        'FlagConteinerTamanho': types.INTEGER(),\n",
    "        'FlagLongoCurso': types.INTEGER(),\n",
    "        'FlagMCOperacaoCarga': types.INTEGER(),\n",
    "        'FlagOffshore': types.INTEGER(),\n",
    "        'FlagTransporteViaInterioir': types.INTEGER(),\n",
    "        'Percurso Transporte em vias Interiores': types.VARCHAR(length=100),\n",
    "        'Percurso Transporte Interiores': types.VARCHAR(length=100),\n",
    "        'STNaturezaCarga': types.VARCHAR(length=100),\n",
    "        'STSH2': types.VARCHAR(length=50),\n",
    "        'STSH4': types.VARCHAR(length=50),\n",
    "        'Natureza da Carga': types.VARCHAR(length=100),\n",
    "        'Sentido': types.VARCHAR(length=50),\n",
    "        'TEU': types.FLOAT(),\n",
    "        'QTCarga': types.INTEGER(),\n",
    "        'VLPesoCargaBruta': types.FLOAT(),\n",
    "        'Carga_Conteinerizada': types.INTEGER(),\n",
    "        'VLPesoCargaConteinerizada': types.FLOAT(),\n",
    "        'Peso_liquido': types.FLOAT()\n",
    "    }\n",
    "\n",
    "    # Nome da tabela no SQL Server\n",
    "    table_name = 'carga_fato'\n",
    "\n",
    "    # Cria a tabela no SQL Server (se não existir) e insere os dados\n",
    "    try:\n",
    "        # Conecta ao banco de dados\n",
    "        with engine.connect() as connection:\n",
    "            # Cria a tabela\n",
    "            df_carga.head(0).to_sql(\n",
    "                name=table_name,\n",
    "                con=connection,\n",
    "                if_exists='replace',\n",
    "                index=False,\n",
    "                dtype=dtype_mapping\n",
    "            )\n",
    "            print(f\"Tabela '{table_name}' criada com sucesso.\")\n",
    "\n",
    "            # Insere os dados na tabela\n",
    "            df_carga.to_sql(\n",
    "                name=table_name,\n",
    "                con=connection,\n",
    "                if_exists='append',\n",
    "                index=False\n",
    "            )\n",
    "            print(f\"Dados inseridos na tabela '{table_name}' com sucesso.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao inserir dados no SQL Server: {e}\")\n",
    "\n",
    "\n",
    "# Definição do DAG\n",
    "with DAG(\n",
    "    \"dag_antaq_etl\",\n",
    "    default_args={\n",
    "        \"owner\": \"airflow\",\n",
    "        \"depends_on_past\": False,\n",
    "        \"email_on_failure\": True,\n",
    "        \"email\": [\"seuemail@dominio.com\"],\n",
    "    },\n",
    "    schedule_interval=\"@monthly\",\n",
    "    start_date=days_ago(1),\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "\n",
    "    extract_task = PythonOperator(\n",
    "        task_id=\"extract_data\",\n",
    "        python_callable=extract_data,\n",
    "    )\n",
    "\n",
    "    transform_task = PythonOperator(\n",
    "        task_id=\"transform_data\",\n",
    "        python_callable=transform_data,\n",
    "    )\n",
    "\n",
    "    load_task = PythonOperator(\n",
    "        task_id=\"load_data\",\n",
    "        python_callable=load_data,\n",
    "    )\n",
    "\n",
    "    load_table_a_task = PythonOperator(\n",
    "        task_id=\"load_table_a\",\n",
    "        python_callable=load_table_a,\n",
    "    )\n",
    "\n",
    "    load_table_c_task = PythonOperator(\n",
    "        task_id=\"load_table_c\",\n",
    "        python_callable=load_table_c,\n",
    "    )\n",
    "\n",
    "    notify_success = EmailOperator(\n",
    "        task_id=\"notify_success\",\n",
    "        to=\"leonardo_rocha18@hotmail.com\",\n",
    "        subject=\"ETL Concluído com Sucesso\",\n",
    "        html_content=\"O processo ETL da ANTAQ foi concluído com sucesso!\"\n",
    "    )\n",
    "\n",
    "    # Definição da ordem de execução das tarefas\n",
    "    extract_task >> transform_task >> load_task >> [load_table_a_task, load_table_c_task] >> notify_success"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
